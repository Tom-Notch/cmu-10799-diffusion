---
data:
    dataset: celeba
    root: ./data/celeba-subset # Your dataset path
    from_hub: false # run "python download_dataset.py --output_dir YOUR_PATH" first if you want to use false here
    repo_name: electronickale/cmu-10799-celeba64-subset
    image_size: 64
    channels: 3
    num_workers: 4
    pin_memory: true
    augment: true

model:
    base_channels: 128
    channel_mult: [1, 2, 2, 4]
    num_res_blocks: 2
    attention_resolutions: [16, 8]
    num_heads: 4
    dropout: 0.1
    use_scale_shift_norm: true

training:
    batch_size: 512 # int
    learning_rate: 2.0e-4 # float
    weight_decay: 1.0e-4 # float
    betas: [0.9, 0.999] # list of float
    ema_decay: 0.9999 # float
    ema_start: 5000 # int # This is the start iter to use EMA during sampling at training time, as using EMA at the beginning may not be very meaningful
    gradient_clip_norm: 1.0 # float
    num_iterations: 20000 # int
    log_every: 100 # int
    sample_every: 1000 # int
    save_every: 1000 # int
    num_samples: 64 # int

ddpm:
    num_timesteps: 1000 # int
    beta_start: 1.0e-4 # float
    beta_end: 2.0e-2 # float

sampling:
    num_steps: 1000 # int
    sampler: ddpm # for the first hw we only need ddpm sampling, this is a placeholder where you can add more options yourself later

infrastructure:
    seed: 42
    device: cuda
    num_gpus: 1 # int # 1 should be ok
    mixed_precision: true # bool # both true and false should work
    compile_model: false # only tested false

checkpoint:
    dir: ./checkpoints
    resume:

logging:
    dir: ./logs
    wandb:
        enabled: true
        project: cmu-10799-diffusion
        entity: tomnotch
